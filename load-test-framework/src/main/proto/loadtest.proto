syntax = "proto3";

package google.pubsub.loadtest;

import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";

option java_package = "com.google.pubsub.flic.common";
option java_outer_classname = "LoadtestProto";

service Loadtest {
  // Starts a load test
  rpc Start(StartRequest) returns (StartResponse);

  // Checks the status of a load test
  rpc Check(CheckRequest) returns (CheckResponse);
}

service LoadtestWorker {
  // Starts a worker
  rpc Start (StartRequest) returns (StartResponse);

  // Executes a command on the worker, returning the latencies of the operations. Since some
  // commands consist of multiple operations (i.e. pulls contain many received messages with
  // different end to end latencies) a single command can have multiple latencies returned.
  rpc Execute (ExecuteRequest) returns (ExecuteResponse);
}

message StartRequest {
  // The GCP project. This must be set even for Kafka, as we use it to export metrics.
  string project = 1;

  // The Pub/Sub or Kafka topic name.
  string topic = 2;

  // The number of requests that can be made, each second, per client.
  int32 request_rate = 3;

  // The size of each user message to publish
  int32 message_size = 4;

  // The maximum outstanding requests, per client.
  int32 max_outstanding_requests = 5;

  // The time at which the load test should start. If this is less than the current time, we start immediately.
  google.protobuf.Timestamp start_time = 6;

  // The burn-in duration, before which results should not be reported.
  google.protobuf.Duration burn_in_duration = 12;

  // The number of user messages of size message_size to publish together.
  int32 publish_batch_size = 11;

  // The max duration for coalescing a batch of published messages.
  google.protobuf.Duration publish_batch_duration = 13;

  oneof stop_conditions {
    // The duration the load test should run for, after burn-in completes.
    google.protobuf.Duration test_duration = 7;

    // The number of messages to publish or expect to receive.
    int32 number_of_messages = 8;
  }

  oneof options {
    PubsubOptions pubsub_options = 9;
    KafkaOptions kafka_options = 10;
  }
}

message StartResponse {
}

message PubsubOptions {
  // The Cloud Pub/Sub subscription name
  string subscription = 1;

  // The maximum number of messages to pull which each request.
  int32 max_messages_per_pull = 2;
}

message KafkaOptions {
  // The network address of the Kafka broker.
  string broker = 1;

  // The length of time to poll for.
  google.protobuf.Duration poll_duration = 2;

  // The network address(es) of the Zookeeper hosts.
  string zookeeper_ip_address = 3;
}

message MessageIdentifier {
  // The unique id of the client that published the message.
  int64 publisher_client_id = 1;

  // Sequence number of the published message with the given publish_client_id.
  int32 sequence_number = 2;
}

message CheckRequest {
  // Duplicate messages that should not be reported for throughput and latency.
  repeated MessageIdentifier duplicates = 1;
}

message CheckResponse {
  // Histogram of latencies, each one a delta from the previous CheckResponse sent.
  repeated int64 bucket_values = 1;

  // The duration from the start of the loadtest to its completion or now if is_finished is false.
  google.protobuf.Duration running_duration = 2;

  // True if the load test has finished running.
  bool is_finished = 3;

  // MessageIdentifiers of all received messages since the last Check
  repeated MessageIdentifier received_messages = 4;
}

message ExecuteRequest {
}

message ExecuteResponse {
  // Latencies of the completed operations
  repeated int64 latencies = 1;

  // MessageIdentifiers of all received messages since the last Execute
  repeated MessageIdentifier received_messages = 2;
}
